# vim: ft=indentcolor

Gradient descent complexity:
    - J(t)
    - dJ(t)/dtj     for j = 0,1,...,n
    - tj := tj - alpha*dJ(t)/dtj

Other optimization algorithm:
    - conjugate gradient
    - BFGS
    - L-BFGS
    - advantages of those algo:
        + no need to manually pick alpha
        + faster than gradient descent
    - disadvantages:
        + more complex

Optimization in Octave:
    - provide:
        function [jVal, gradient] = costFunction(theta)
            jVal = [...code to compute J(theta)...];
            gradient = [...code to compute derivative of J(theta)...];
        end
    - call:
        options = optimset('GradObj', 'on', 'MaxIter', 100);
        initialTheta = zeros(2,1);
        [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);
